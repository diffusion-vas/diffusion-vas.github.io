
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="StyleSheet" href="css/projects.css" type="text/css" media="all">
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
  <title>Using Diffusion Priors for Video Amodal Segmentation</title>
</head>

<body>

  <div class="content content-title" style="text-align: center">
    <h1>Using Diffusion Priors for Video Amodal Segmentation</h1>
    <h3>Kaihua Chen, Deva Ramanan, Tarasha Khurana</h3>
    <h3>Video Versions of Paper Figures</h3>
  </div>


  <div class="content">
        <div style="text-align: center; padding-bottom:20px">
            <h3>Figure 1</h3>
        </div>
        
    <figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
        <video autoplay="" loop="" muted="" playsinline="" width="1000">
            <source src="vids/figure_1.mp4" type="video/mp4">
        </video>
      <figcaption>
        In this work, we tackle the problem of video amodal segmentation and content completion: given a modal (visible) object sequence in a video, we develop a two-stage method that generates its amodal (visible + invisible) masks and RGB content. We capitalize on the shape and temporal consistency priors baked into video foundation models because of their large-scale pretraining. Finetuning these models enables us to infer complete shapes and RGB details of objects that undergo occlusion. Our method is effectively able to handle severe occlusions and generalizes across diverse object categories, achieving state-of-the-art results on synthetic and real-world datasets. 
        We show one such example of an unseen deformable object category `laptop' that undergoes a complete occlusion.
      </figcaption>
    </figure>
  </div>

  <div class="content">
        <div style="text-align: center; padding-bottom:20px">
            <h3>Figure 2</h3>
        </div>
        <!-- <table width=1000px style="text-align: center">
                <tr>
                    <td>Historical LiDAR Sweeps</td>
                    <td>Future 4D Occupancy</td>
                    <td>Rendered Point Clouds</td>
                    <td>Groundtruth Point Clouds</td>
                </tr>
        </table> -->
    <figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
        <video autoplay="" loop="" muted="" playsinline="" width="1000" style="padding-bottom: 20px">
            <source src="vids/figure_2.mp4" type="video/mp4">
        </video>
        <!-- <img src="figs/figure_3_footer.png" width="1000px" style="padding-bottom: 20px"> -->
      <figcaption>
        <b>Model pipeline</b> for amodal segmentation and content completion. The first stage of our pipeline generates amodal masks {A<sub>t</sub>} for an object, given its modal masks {M<sub>t</sub>} and pseudo-depth of the scene {D<sub>t</sub>} (which is obtained by running a monocular depth estimator on RGB video sequence {I<sub>t</sub>} ). The predicted amodal masks from the first stage are then sent as input to the second stage, along with the modal RGB content of the occluded object in consideration. The second stage then inpaints the occluded region and outputs the amodal RGB content {C<sub>t</sub>}
           for the occluded object. Both stages employ a conditional latent diffusion framework with a 3D UNet backbone. Conditionings are encoded via a VAE encoder into latent space, concatenated, and processed by a 3D UNet with interleaved spatial and temporal blocks. CLIP embeddings of {M<sub>t</sub>} and the modal RGB content {V<sub>O</sub>}
            provide cross-attention cues for the first and second stage respectively. Finally, the VAE decoder translates outputs back to pixel space.
      
      </figcaption>
    </figure>
  </div>

  <div class="content">
        <div style="text-align: center; padding-bottom:20px">
            <h3>Figure 3</h3>
        </div>
        <!-- <table width=1000px style="text-align: center">
                <tr>
                    <td width=200px>Groundtruth</td>
                    <td width=200px>SPFNet-U</td>
                    <td width=200px>S2Net-U</td>
                    <td width=200px>Ours (Point clouds)</td>
                    <td width=200px>Ours (Occupancy)</td>
                </tr>
        </table> -->
    <figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
        <video autoplay="" loop="" muted="" playsinline="" width="1000" style="padding-bottom: 20px">
            <source src="vids/figure_3.mp4" type="video/mp4">
        </video>
      <figcaption>
        <b>Modal-amodal RGB training pair.</b> for content completion. The left frame displays the partially occluded modal RGB content, generated by overlaying amodal masks (black regions) onto the amodal object to disrupt its visual integrity. The right frame shows the original, unoccluded amodal RGB object.
       </figcaption>
    </figure>
  </div>

  <div class="content">
        <div style="text-align: center; padding-bottom:20px">
            <h3>Figure 5</h3>
        </div>
        <table width=1000px style="text-align: center">
                <tr>
                    
                    <td width=200px>Modal Content</td>
                    <td width=200px>pix2gestalt</td>
                    <td width=200px>Ours</td>
                </tr>
        </table>
    <figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
        <video autoplay="" loop="" muted="" playsinline="" width="1000" style="padding-bottom: 20px">
            <source src="vids/figure_5.mp4" type="video/mp4">
        </video>
      <figcaption>
        <b>Temporal consistency comparison</b> with an image amodal segmentation method. We highlight the lack of temporal coherence in a single-frame diffusion based method, pix2gestalt, for both the predicted amodal segmentation mask and the RGB content for the occluded person in the example shown. By leveraging temporal priors, our approach achieves significantly higher temporal consistency across occlusions.
      </figcaption>
    </figure>
  </div>


  <div class="content">
    <div style="text-align: center; padding-bottom:20px">
        <h3>Figure 6.1</h3>
    </div>
    <table width=1000px style="text-align: center">
            <tr>
                
                <td width=200px>RGB Image</td>
                <td width=200px>Modal</td>
                <td width=200px>VideoMAE</td>
                <td width=200px>PCNet-M</td>
                <td width=200px>Ours</td>
                <td width=200px>Amodal-GT</td>
            </tr>
    </table>
<figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
    <video autoplay="" loop="" muted="" playsinline="" width="1000" style="padding-bottom: 20px">
        <source src="vids/figure_6_1.mp4" type="video/mp4">
    </video>
  <figcaption>
    <b>Qualitative comparison</b> of amodal segmentation methods on SAIL-VOS and TAO-Amodal. Our method leverages strong shape priors, such as for humans and chairs, to generate clean and realistic object shapes. It also excels in handling heavy occlusions; even when objects are nearly fully occluded (e.g., "chair" in the second row), our method achieves high-fidelity shape completion by utilizing temporal priors. Note that TAO-Amodal contains out-of-frame occlusions which none of the methods are trained for, but our method is able to handle such cases. 
  </figcaption>
</figure>
</div>

<div class="content">
  <div style="text-align: center; padding-bottom:20px">
      <h3>Figure 6.2</h3>
  </div>
  <table width=1000px style="text-align: center">
          <tr>
              
              <td width=200px>RGB Image</td>
              <td width=200px>Modal</td>
              <td width=200px>VideoMAE</td>
              <td width=200px>EoRaS</td>
              <td width=200px>Ours</td>
              <td width=200px>Amodal-GT</td>
          </tr>
  </table>
<figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
  <video autoplay="" loop="" muted="" playsinline="" width="1000" style="padding-bottom: 20px">
      <source src="vids/figure_6_2.mp4" type="video/mp4">
  </video>
<figcaption>
  <b>Qualitative comparison</b> of amodal segmentation methods on MOVi-B/D. Our method leverages robust shape priors for boots and teapots, ensuring consistent shapes even under significant camera movement and near-complete occlusion of the objects.
</figcaption>
</figure>
</div>





  <div class="content">
        <div style="text-align: center; padding-bottom:20px">
            <h3>Figure 7</h3>
        </div>
    <figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
        <video autoplay="" loop="" muted="" playsinline="" width="1000" style="padding-bottom: 20px">
            <source src="vids/figure_7.mp4" type="video/mp4">
        </video>
      <figcaption>
        <b> Qualitative results for content completion.</b> Although our content completion module, initialized from pretrained SVD weights, is finetuned solely on synthetic SAIL-VOS, it achieves photorealistic, high-fidelity object inpainting even in real-world scenarios. Furthermore, our method can complete unseen categories, such as giraffes and plastic bottle, likely due to its ability to transfer styles and patterns from the visible parts of objects to occluded areas in the current or neighboring frames. We show examples from TAO-Amodal (top) and in-the-wild YouTube videos (bottom).
      </figcaption>
    </figure>
  </div>


  <div class="content">
    <div style="text-align: center; padding-bottom:20px">
        <h3>Figure 8</h3>
    </div>
<figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
    <video autoplay="" loop="" muted="" playsinline="" width="1000" style="padding-bottom: 20px">
        <source src="vids/figure_8.mp4" type="video/mp4">
    </video>
  <figcaption>
    We show an example of <b>multi-modal generation</b> from our diffusion model. Since there are multiple plausible explanations for the shape of the person in his occluded region, our model predicts two such plausible amodal masks (with the person's occluded legs in two different orientations).
  </figcaption>
</figure>
</div>



<div class="content">
  <div style="text-align: center; padding-bottom:20px">
      <h3>Figure 11</h3>
  </div>
  <table width=1000px style="text-align: center">
    <tr>
        
        <td width=200px>w/o ours</td>
    </tr>
</table>
<figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
  <video autoplay="" loop="" muted="" playsinline="" width="1000">
      <source src="vids/figure_11_1.mp4" type="video/mp4">
  </video>
  <table width=1000px style="text-align: center">
    <tr>
        <td width=200px>w/ ours</td>
    </tr>
</table>

<table width=1000px style="text-align: center">
  <tr>
      
      <td width=200px>w/o ours</td>
  </tr>
</table>
<figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
<video autoplay="" loop="" muted="" playsinline="" width="1000">
    <source src="vids/figure_11_2.mp4" type="video/mp4">
</video>
<table width=1000px style="text-align: center; padding-bottom:20px">
  <tr>
      <td width=200px>w/ ours</td>
  </tr>
</table>

<figcaption>
  <b> 4D reconstruction results </b>. Without amodal completion by our method, the 4D reconstruction exhibits blank regions and unrealistic artifacts in occluded areas, such as the person’s back and leg. The varying occluded portions over time confuse SV4D, disrupting its understanding of the object's 3D structure. In contrast, using completed objects from our method significantly improves the reconstruction quality, producing more consistent and clear novel-views.</figcaption>
</figure>
</div>




<div class="content">
  <div style="text-align: center; padding-bottom:20px">
      <h3>Figure 12</h3>
  </div>
  <table width=1000px style="text-align: center">
    <tr>
        
        <td width=200px>Source</td>
        <td width=200px>Manipulated</td>
    </tr>
</table>
<figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
  <video autoplay="" loop="" muted="" playsinline="" width="1000" style="padding-bottom: 20px">
      <source src="vids/figure_12.mp4" type="video/mp4">
  </video>
<figcaption>
  <b> Scene manipulation examples </b>. Using de-occluded objects from our method, we can reposition and reorder them to create new scenes. In the top rows, the relationship between the person and the soccer ball is altered, changing the scene from “the person is juggling” to “the person places the soccer ball aside and practices a juggling posture.” In the bottom rows, the middle giraffe is moved to the front and its position is adjusted.
</figcaption>
</figure>
</div>



<div class="content">
  <div style="text-align: center; padding-bottom:20px">
      <h3>Figure 13</h3>
  </div>
<figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
  <video autoplay="" loop="" muted="" playsinline="" width="1000" style="padding-bottom: 20px">
      <source src="vids/figure_13.mp4" type="video/mp4">
  </video>
<figcaption>
  <b> Qualitative results for pseudo-ground truth of TAO-Amodal masks </b>.  Leveraging the amodal bounding box as a strong prior, our method demonstrates versatility across diverse categories, such as person, tractor, and bottles, and generalizes well to unseen categories like snowboards and horses. This high-quality pseudo-ground truth can semi-automate the manual annotation of amodal masks in real-world videos.
</figcaption>
</figure>
</div>

<div class="content">
  <div style="text-align: center; padding-bottom:20px">
      <h3>Figure 15 & 16</h3>
  </div>

  <table width=1000px style="text-align: center">
    <tr>
        
        <td width=200px>RGB Image</td>
        <td width=200px>Modal</td>
        <td width=200px>Convex</td>
        <td width=200px>Convex<sup>R</sup></td>
        <td width=200px>PCNet-M</td>
    </tr>
</table>
<figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
  <video autoplay="" loop="" muted="" playsinline="" width="1000">
      <source src="vids/figure_15&16.mp4" type="video/mp4">
  </video>

  <table width=1000px style="text-align: center; padding-bottom:20px">
    <tr>
        
        <td width=200px>pix2gestalt</td>
        <td width=200px>VideoMAE</td>
        <td width=200px>3D UNet</td>
        <td width=200px>Ours</td>
        <td width=200px>Amodal-GT</td>
    </tr>
</table>
<figcaption>
  <b> Qualitative results on SAIL-VOS </b>.
</figcaption>
</figure>
</div>

<div class="content">
  <div style="text-align: center; padding-bottom:20px">
      <h3>Figure 17 & 18</h3>
  </div>
<figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">

  <table width=1000px style="text-align: center">
    <tr>
        
        <td width=200px>RGB Image</td>
        <td width=200px>Modal</td>
        <td width=200px>PCNet-M</td>
        <td width=200px>pix2gestalt</td>
    </tr>
</table>

  <video autoplay="" loop="" muted="" playsinline="" width="1000">
      <source src="vids/figure_17&18.mp4" type="video/mp4">
  </video>
  <table width=1000px style="text-align: center; padding-bottom:20px" >
    <tr>
        <td width=200px>VideoMAE</td>
        <td width=200px>3D UNet</td>
        <td width=200px>Ours</td>
        <td width=200px>Amodal-GT</td>
    </tr>
</table>
<figcaption>
  <b> Qualitative results on TAO-Amodal </b>.
</figcaption>
</figure>
</div>

<div class="content">
  <div style="text-align: center; padding-bottom:20px">
      <h3>Figure 19</h3>
  </div>
  <table width=1000px style="text-align: center">
    <tr>
      <td width=200px>RGB Image</td>
        <td width=200px>Modal</td>  
        <td width=200px>VideoMAE</td>
        <td width=200px>EoRaS</td>
        <td width=200px>Ours</td>
        <td width=200px>Amodal-GT</td>
    </tr>
</table>
<figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
  <video autoplay="" loop="" muted="" playsinline="" width="1000" style="padding-bottom: 20px">
      <source src="vids/figure_19.mp4" type="video/mp4">
  </video>
<figcaption>
  <b> Qualitative results on MOVi-B/D </b>.
</figcaption>
</figure>
</div>




<div class="content">
  <div style="text-align: center; padding-bottom:20px">
      <h3>Figure 20</h3>
  </div>
<figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
  <video autoplay="" loop="" muted="" playsinline="" width="1000" style="padding-bottom: 20px">
      <source src="vids/figure_20.mp4" type="video/mp4">
  </video>
<figcaption>
  <b> Qualitative results for amodal content completion for in-the-wild scenarios </b>.
</figcaption>
</figure>
</div>





</html>

