<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="We tackle the problem of video amodal segmentation and content completion using diffusion priors.">
  <meta name="keywords" content="video amodal segmentation, video diffusion, amodal segmentation, content completion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Using Diffusion Priors for Video Amodal Segmentation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/diffvasicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Using Diffusion Priors for Video Amodal Segmentation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/kaihuac/">Kaihua Chen</a>,&nbsp;</span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>,&nbsp;</span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~tkhurana/">Tarasha Khurana</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Carnegie Mellon University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2412.04623"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.04623"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=ilHCPNaH7tY"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Kaihua-Chen/diffusion-vas"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="./page1/index.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-video"></i>
                  </span>
                  <span>Figures (Video)</span>
                </a>
              </span>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle">
        We tackle the problem of video amodal segmentation and content completion: given a modal (visible) object sequence in a video,
         we develop a two-stage method that generates its amodal (visible + invisible) masks and RGB content.
          Here, we show one such example of an unseen deformable object category 'laptop' that undergoes a complete occlusion.
      </h2>
    </div>
  </div>
</section>

<div class="columns is-centered has-text-centered">
<h2 class="title is-3">In-the-wild Gallery</h2>
</div>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-horse">
          <video poster="" id="horse" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/horse.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-bird">
          <video poster="" id="bird" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/bird.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-soccer">
          <video poster="" id="soccer" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/soccer.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-trump">
          <video poster="" id="trump" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/trump.mp4"
                    type="video/mp4">
          </video>
        </div>
        
        <div class="item item-weird">
          <video poster="" id="weird" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/weird.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="item item-bottle">
          <video poster="" id="bottle" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/bottle.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="item item-tourist">
          <video poster="" id="tourist" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tourist.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="item item-chef">
          <video poster="" id="chef" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chef.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="item item-giraffe">
          <video poster="" id="giraffe" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/giraffe.mp4"
                    type="video/mp4">
          </video>
        </div>
      
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Object permanence in humans is a fundamental cue that helps in understanding persistence of objects, even when they are fully occluded in the scene. 
            Present day methods in object segmentation do not account for this <i>amodal</i> nature of the world, and only work for segmentation of visible or <i>modal</i> objects. 
            Few amodal methods exist; single-image segmentation methods cannot handle high-levels of occlusions which are better inferred using temporal information, 
            and multi-frame methods have focused solely on segmenting rigid objects. 
          </p>
          <p>
            To this end, we propose to tackle video amodal segmentation by formulating it as a conditional generation task, 
            capitalizing on the foundational knowledge in video generative models. Our method is simple; we repurpose these models to condition on a sequence of modal mask frames of an object along with contextual pseudo-depth maps, 
            to learn which object boundary may be occluded and therefore, extended to hallucinate the complete extent of an object. This is followed by a content completion stage which is able to inpaint the occluded regions of an object.
          </p>
            <p>
            We benchmark our approach alongside a wide array of state-of-the-art methods on four datasets and show a dramatic improvement of upto 13% for amodal segmentation in an object's occluded region. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/ilHCPNaH7tY?rel=0&amp;showinfo=0" 
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>



<section class="section">

  <div class="container is-max-desktop" style="margin-bottom: 30px;">

    <div class="columns is-centered">

      <!-- Comparison on SAIL-VOS. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Comparison on SAIL-VOS</h2>
          <p>
            We evaluate our segmentation method on SAIL-VOS dataset. 
            Our model achieves high-fidelity shape completion across diverse categories,
             handling both rigid and deformable objects effectively.
          </p>

          <table width=1000px style="text-align: center; margin-bottom: -10px;">
            <tr>
                
                <td width=200px>RGB Image</td>
                <td width=200px>Modal</td>
                <td width=200px>Convex</td>
                <td width=200px>Convex<sup>R</sup></td>
                <td width=200px>PCNet-M</td>
            </tr>
        </table>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/sailvos.mp4"
                    type="video/mp4">
          </video>
          <table width=1000px style="text-align: center; margin-top: -14px;">
            <tr>
                
                <td width=200px>pix2gestalt</td>
                <td width=200px>VideoMAE</td>
                <td width=200px>3D UNet</td>
                <td width=200px>Ours</td>
                <td width=200px>Amodal-GT</td>
            </tr>
        </table>
        </div>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop"  style="margin-bottom: 30px;">

    <div class="columns is-centered">

      <!-- Comparison on TAO-Amodal. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Comparison on TAO-Amodal</h2>
          <p>
            For zero-shot evaluation, we test on the real-world TAO-Amodal dataset. 
             Despite being trained exclusively on synthetic data, 
             our model generalizes well to real-world scenarios, even for unseen object categories.
          </p>

          <table width=1000px style="text-align: center; margin-bottom: -10px;">
            <tr>
                
                <td width=200px>RGB Image</td>
                <td width=200px>Modal</td>
                <td width=200px>PCNet-M</td>
                <td width=200px>pix2gestalt</td>
            </tr>
        </table>
          <video id="tao_amodal" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tao_amodal.mp4"
                    type="video/mp4">
          </video>
          <table width=1000px style="text-align: center; margin-top: -14px;">
            <tr>
                
                <td width=200px>VideoMAE</td>
                <td width=200px>3D UNet</td>
                <td width=200px>Ours</td>
                <td width=200px>Amodal-GT</td>
            </tr>
        </table>
        </div>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Comparison on MOVi-B/D. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Comparison on MOVi-B/D</h2>
          <p>
            We also benchmark our method on the MOVi-B and MOVi-D datasets. These datasets present challenges such as strong camera motion and frequent full occlusion.
             Our method consistently outperforms state-of-the-art baselines,
              maintaining robustness <i>without</i> relying on additional inputs like camera poses or optical flow.
          </p>

          <table width=1000px style="text-align: center; margin-bottom: -10px;">
            <tr>
                
                <td width=200px>RGB Image</td>
                <td width=200px>Modal</td>
                <td width=200px>VideoMAE</td>
                <td width=200px>EoRaS</td>
                <td width=200px>Ours</td>
                <td width=200px>Amodal-GT</td>
            </tr>
        </table>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/movi.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>

</section>


    <!--/ Pseudo-gt. -->


    <!-- Concurrent Work. -->

    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          Coming soon!
        </div>
      </div>
    </div> -->

    <!--/ Concurrent Work. -->



<section class="section">


  <div class="container is-max-desktop">
      <!-- How does it work?. -->
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">How does it work?</h2>
          <p>
            The first stage of our pipeline generates amodal masks {A<sub>t</sub>} for an object, given its modal masks {M<sub>t</sub>} and pseudo-depth of the scene {D<sub>t</sub>} (which is obtained by running a monocular depth estimator on RGB video sequence {I<sub>t</sub>} ). The predicted amodal masks from the first stage are then sent as input to the second stage, along with the modal RGB content of the occluded object in consideration. The second stage then inpaints the occluded region and outputs the amodal RGB content {C<sub>t</sub>}
           for the occluded object. Both stages employ a conditional latent diffusion framework with a 3D UNet backbone.
          </p>
          <video id="method" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/method.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ How does it work?. -->
    </div>

  </div>


</section>

  



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{chen2024diffvas,
  author    = {Chen, Kaihua and Ramanan, Deva and Khurana, Tarasha},
  title     = {Using Diffusion Priors for Video Amodal Segmentation},
  journal   = {arXiv preprint arXiv:},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/Kaihua-Chen" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content" style="text-align: center;">
          <p>
            This website is borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. We sincerely thank them for this excellent open-source template!
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
